import com.google.common.base.Stopwatch;
import com.google.gson.GsonBuilder;
import edu.stanford.nlp.simple.Sentence;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;

import java.io.File;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.*;
import java.util.concurrent.ConcurrentSkipListMap;
import java.util.concurrent.ConcurrentSkipListSet;
import java.util.logging.Level;
import java.util.logging.Logger;

/*
 * This Java source file was auto generated by running 'gradle buildInit --type java-library'
 * by 'koray2' at '4/8/16 5:56 PM' with Gradle 2.6
 *
 * @author koray2, @date 4/8/16 5:56 PM
 */
public class Bil681Project {

     private static final String [] LUCENE_STOP_WORDS = new String[]{
             "a", "an", "and", "are", "as", "at", "be", "but", "by",
            "for", "if", "in", "into", "is", "it",
            "no", "not", "of", "on", "or", "such",
            "that", "the", "their", "then", "there", "these",
            "they", "this", "to", "was", "will", "with"};


    private static final String OUTPUT_FOLDER = "out"; // relative to project root

    private static final String INVERTED_INDEX_FILE_NAME = "inverted_index.json"; // relative to output folder

    private static final Logger LOGGER = Logger.getLogger(Bil681Project.class.getName());

    private static final int RARITY_CUT_OFF_LEVEL = 5;

    public static final String RAW_DATA_DIRECTORY = "data"; // relative to project root

    Map<String , SortedSet<Posting>> invertedIndex;

    List<edu.stanford.nlp.simple.Document> nlpDocuments;


    public static void main(String... args) throws IOException {
        Bil681Project project = new Bil681Project();
        Stopwatch timer = Stopwatch.createStarted();
        List<Document> documents = project.memoizeDocumentsUnderPath(Paths.get(RAW_DATA_DIRECTORY));
        LOGGER.log(Level.INFO , "Taking documents to memory took : " + timer.stop().toString());
        timer = Stopwatch.createStarted();
        List<RecipeData> recipeDatas = project.parseDocuments(documents);
        LOGGER.log(Level.INFO , "Parsing documents took : " + timer.stop().toString());
        project.generateNlpDocuments(recipeDatas);
        timer = Stopwatch.createStarted();
        project.generateInvertedIndex();
        LOGGER.log(Level.INFO , "Inverted index generation took : " + timer.stop().toString());
        project.filterRareWords(RARITY_CUT_OFF_LEVEL);
        new File(OUTPUT_FOLDER).mkdirs();
        Files.write(Paths.get(OUTPUT_FOLDER , INVERTED_INDEX_FILE_NAME) , new GsonBuilder().setPrettyPrinting().create().toJson(project.invertedIndex).getBytes()  );

    }


    /**
     * Removes rare words from inverted index
     * @param rarityCutOffLevel number of occurences required for dictionary term to survive
     */
    private void filterRareWords(int rarityCutOffLevel) {
        for(Map.Entry<String , SortedSet<Posting>> entry : invertedIndex.entrySet()){
            if(entry.getValue().size() < rarityCutOffLevel )
                invertedIndex.remove(entry.getKey());
        }
    }

    private void generateInvertedIndex() {
        List<String> stopWordList = Arrays.asList(LUCENE_STOP_WORDS);
        invertedIndex = new ConcurrentSkipListMap<>();
        for( int i = 0 ; i < nlpDocuments.size() ; i++)
        {
            for(Sentence sentence : nlpDocuments.get(i).sentences()){
                for (String s : sentence.lemmas())
                {
                    String lowercase = s.toLowerCase(Locale.ENGLISH);
                    if(lowercase.matches("[A-Za-z]+") && !stopWordList.contains(lowercase))
                    {
                        SortedSet<Posting> postings = invertedIndex.get(lowercase);
                        if(!invertedIndex.containsKey(lowercase)){
                            postings = new ConcurrentSkipListSet<>();
                            invertedIndex.put(lowercase , postings);
                        }
                        postings.add(new Posting(i));
                    }
                }
            }
        }

    }

    /**
     * takes generic recipe data and returns nlp document objects
     * @return
     */
    public void generateNlpDocuments(List<RecipeData> recipeDatas ) {
        nlpDocuments = new ArrayList<>();
        for(int i = 0 ; i < recipeDatas.size() ; i++){
            edu.stanford.nlp.simple.Document doc = new edu.stanford.nlp.simple.Document(recipeDatas.get(i).toPlainString());
            doc.setDocid(Integer.toString(i));
            nlpDocuments.add(doc);
        }
    }

    /**
     * parses each file in directory to in memory Jsoup documents
     *
     * @param directory
     * @return list of discovered jsoup documents
     * @throws IOException
     */
    public List<Document> memoizeDocumentsUnderPath(Path directory) throws IOException {
        final List<Document> documents = new ArrayList<Document>();
        Files.list(directory).forEach((k) -> {
            try {
                documents.add(Jsoup.parse(k.toFile(), "UTF-8"));
//                LOGGER.log(Level.INFO , "memoized file "+ k.toString());
            } catch (IOException e) {
                e.printStackTrace();
            }
        });
        return documents;
    }

    /**
     * Parses to extract specific data into custom java pojo from recipe document. Expects very specific data format.
     * TODO find out how category field can be parsed
     * @param documents list of allrecipe.com recipe html jsoup document
     * @return list of RecipeData objects
     */
    public List<RecipeData> parseDocuments(List<Document> documents) {
        List<RecipeData> recipeDatas = new ArrayList<>();
        for (Document d : documents) {
            try {
                RecipeData recipeData = new RecipeData();
                recipeData.title = d.getElementsByClass("recipe-summary__h1").html();
                recipeData.description = d.getElementsByClass("submitter__description").html();
                recipeData.submitter = d.getElementsByClass("submitter__name").html();
                recipeData.ingredients = d.getElementsByAttributeValue("itemprop", "ingredients").html().replace("\n", ". ");
                recipeData.servings = d.getElementById("metaRecipeServings").attr("content");
                recipeData.duration = d.getElementsByClass("ready-in-time").html();
                recipeData.nutrition = d.getElementsByClass("calorie-count").get(0).child(0).text();
                recipeData.directions = d.getElementsByClass("recipe-directions__list--item").html().replace("\n", ". ");
//            LOGGER.log(Level.INFO, recipeData.toString());
                recipeDatas.add(recipeData);
            }catch (NullPointerException | IndexOutOfBoundsException e){
//                LOGGER.log(Level.SEVERE , "Failed to parse a file, omitting from document set.");

            }
        }
        return recipeDatas;
    }


    /**
     * just a project specific pojo
     */
    public class RecipeData {
        public String title, description, submitter, ingredients, servings, duration, directions, nutrition, category;

        @Override
        public String toString() {
            //don't try this at home!
            return new GsonBuilder().setPrettyPrinting().create().toJson(this);
        }

        public String toPlainString() {
            return title + " " + description + " " + submitter + " " + ingredients + " " + servings + " " + duration + " " + directions + " " + nutrition;
        }
    }

}
